{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "274e3108",
   "metadata": {},
   "source": [
    "# TPC3: Analisador Léxico para SPARQL\n",
    "\n",
    "Este trabalho implementa um tokenizador (analisador léxico) para queries SPARQL, capaz de reconhecer:\n",
    "- Palavras-chave (SELECT, WHERE, PREFIX, OPTIONAL, FILTER)\n",
    "- Variáveis (?nome, ?desc)\n",
    "- Identificadores (:Pessoa, :temIdade)\n",
    "- Strings, números e operadores\n",
    "- Pontuação e estrutura da query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b44827",
   "metadata": {},
   "source": [
    "Exemplo de Query\n",
    "\n",
    "# DBPedia: obras de Chuck Berry\n",
    "\n",
    "select ?nome ?desc where { \n",
    "    ?s a dbo:MusicalArtist. \n",
    "    ?s foaf:name \"Chuck Berry\"@en . \n",
    "    ?w dbo:artist ?s. \n",
    "    ?w foaf:name ?nome. \n",
    "    ?w dbo:abstract ?desc \n",
    "} LIMIT 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b173376",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SELECT', 'SELECT', 1, (0, 6)),\n",
       " ('VAR', '?ola', 1, (7, 11)),\n",
       " ('VAR', '?jorge', 1, (12, 18)),\n",
       " ('VAR', '?tudo', 1, (19, 24)),\n",
       " ('WHERE', 'WHERE', 1, (26, 31)),\n",
       " ('NEWLINE', '\\n', 2, (32, 33)),\n",
       " ('PUNCT', '{', 2, (33, 34)),\n",
       " ('NEWLINE', '\\n', 3, (34, 35)),\n",
       " ('VAR', '?ola', 3, (37, 41)),\n",
       " ('ERROR', 'a', 3, (42, 43)),\n",
       " ('IDENT', ':Pessoa', 3, (44, 51)),\n",
       " ('PUNCT', ';', 3, (52, 53)),\n",
       " ('NEWLINE', '\\n', 4, (53, 54)),\n",
       " ('IDENT', ':temIdade', 4, (59, 68)),\n",
       " ('ERROR', '?', 4, (69, 70)),\n",
       " ('INT', '19', 4, (70, 72)),\n",
       " ('PUNCT', ';', 4, (73, 74)),\n",
       " ('NEWLINE', '\\n', 5, (74, 75)),\n",
       " ('IDENT', ':eIrmaoDe', 5, (80, 89)),\n",
       " ('VAR', '?jorge', 5, (90, 96)),\n",
       " ('PUNCT', '.', 5, (97, 98)),\n",
       " ('NEWLINE', '\\n', 6, (98, 99)),\n",
       " ('PUNCT', '}', 6, (99, 100)),\n",
       " ('NEWLINE', '\\n', 7, (100, 101))]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tpc3(teste):\n",
    "    tokens=[\n",
    "    ('NEWLINE',r'\\n'),\n",
    "    ('SKIP',r'[ \\t]+'),\n",
    "    ('PREFIX',r'PREFIX\\b'),\n",
    "    ('SELECT',r'SELECT\\b'),\n",
    "    ('WHERE',r'WHERE\\b'),\n",
    "    ('OPTIOMAL',r'OPTIONAL\\b'),\n",
    "    ('FILTER',r'FILTER\\b'),\n",
    "    ('VAR',r'\\?[a-zA-Z_][\\w]*'),\n",
    "    ('URI',r'<[^>]*>'),\n",
    "    ('IDENT',r':[a-zA-Z_][\\w]*'),\n",
    "    ('INT',r'\\d+'),\n",
    "    ('STRING',r'\"[^\"]*\"'),\n",
    "    ('OP',r'[=!<>]+'),\n",
    "    ('PUNCT',r'[{}.;,]'),\n",
    "    ('ERROR',r'.')\n",
    "]\n",
    "    reconhecidos=[]\n",
    "    linha=1 \n",
    "    tokenreg='|'.join(f'(?P<{name}>{pattern})'for name , pattern in tokens) \n",
    "    val=re.finditer(tokenreg,teste)\n",
    "    for v in val:\n",
    "        dic=v.groupdict()\n",
    "        type=None\n",
    "        valor=v.group()\n",
    "        for key in dic:\n",
    "            if dic[key]:\n",
    "                tipo=key\n",
    "                break\n",
    "        if tipo=='NEWLINE':\n",
    "            linha+=1\n",
    "            reconhecidos.append((tipo, valor, linha, v.span()))\n",
    "        elif tipo != 'SKIP':\n",
    "            reconhecidos.append((tipo, valor, linha, v.span()))\n",
    "            \n",
    "    return reconhecidos\n",
    "    print (reconhecidos)\n",
    "    \n",
    "teste=\"\"\"SELECT ?ola ?jorge ?tudo  WHERE \n",
    "{\n",
    "  ?ola a :Pessoa ;\n",
    "     :temIdade ?19 ;\n",
    "     :eIrmaoDe ?jorge .\n",
    "}\n",
    "\"\"\"\n",
    "tpc3(teste)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520a35a8",
   "metadata": {},
   "source": [
    "## Implementação do Tokenizador\n",
    "\n",
    "O tokenizador usa expressões regulares para identificar cada tipo de token na query SPARQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d08a441",
   "metadata": {},
   "source": [
    "## Exemplo de Teste\n",
    "\n",
    "Vamos testar o tokenizador com uma query SPARQL simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4805ac1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SELECT', 'SELECT', 1, (0, 6))\n",
      "('VAR', '?ola', 1, (7, 11))\n",
      "('VAR', '?jorge', 1, (12, 18))\n",
      "('VAR', '?tudo', 1, (19, 24))\n",
      "('WHERE', 'WHERE', 1, (26, 31))\n",
      "('NEWLINE', '\\n', 2, (32, 33))\n",
      "('PUNCT', '{', 2, (33, 34))\n",
      "('NEWLINE', '\\n', 3, (34, 35))\n",
      "('VAR', '?ola', 3, (37, 41))\n",
      "('ERROR', 'a', 3, (42, 43))\n"
     ]
    }
   ],
   "source": [
    "# Executar o tokenizador\n",
    "resultado = tpc3(teste)\n",
    "\n",
    "# Mostrar os primeiros tokens reconhecidos\n",
    "for token in resultado[:10]:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd25444f",
   "metadata": {},
   "source": [
    "## Resultado Esperado\n",
    "\n",
    "Cada token é representado como uma tupla com:\n",
    "- **Tipo**: categoria do token (SELECT, VAR, PUNCT, etc.)\n",
    "- **Valor**: texto reconhecido\n",
    "- **Linha**: número da linha onde o token aparece\n",
    "- **Posição**: tupla (início, fim) indicando a posição no texto original"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
